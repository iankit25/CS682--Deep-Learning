{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Image features exercise\n",
    "\n",
    "I have used dense SIFT features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from asgn1.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Similar to previous exercises, we will load CIFAR-10 data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from asgn1.features import color_histogram_hsv, hog_feature\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # Subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "\n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    " making use of dSIFT features code for the paper :Y. Jia and T. Darrell. ``Heavy-tailed Distances for Gradient Based Image Descriptors''. NIPS 2011.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images extracted\n",
      "1000 images extracted\n",
      "2000 images extracted\n",
      "3000 images extracted\n",
      "4000 images extracted\n",
      "5000 images extracted\n",
      "6000 images extracted\n",
      "7000 images extracted\n",
      "8000 images extracted\n",
      "9000 images extracted\n",
      "10000 images extracted\n",
      "11000 images extracted\n",
      "12000 images extracted\n",
      "13000 images extracted\n",
      "14000 images extracted\n",
      "15000 images extracted\n",
      "16000 images extracted\n",
      "17000 images extracted\n",
      "18000 images extracted\n",
      "19000 images extracted\n",
      "20000 images extracted\n",
      "21000 images extracted\n",
      "22000 images extracted\n",
      "23000 images extracted\n",
      "24000 images extracted\n",
      "25000 images extracted\n",
      "26000 images extracted\n",
      "27000 images extracted\n",
      "28000 images extracted\n",
      "29000 images extracted\n",
      "30000 images extracted\n",
      "31000 images extracted\n",
      "32000 images extracted\n",
      "33000 images extracted\n",
      "34000 images extracted\n",
      "35000 images extracted\n",
      "36000 images extracted\n",
      "37000 images extracted\n",
      "38000 images extracted\n",
      "39000 images extracted\n",
      "40000 images extracted\n",
      "41000 images extracted\n",
      "42000 images extracted\n",
      "43000 images extracted\n",
      "44000 images extracted\n",
      "45000 images extracted\n",
      "46000 images extracted\n",
      "47000 images extracted\n",
      "48000 images extracted\n",
      "0 images extracted\n",
      "0 images extracted\n",
      "(1000, 1152)\n"
     ]
    }
   ],
   "source": [
    "from dsift import *\n",
    "from scipy import misc\n",
    "extractor = DsiftExtractor(8,16,1)\n",
    "fe=[]\n",
    "sha=[X_train.shape[0],X_val.shape[0],X_test.shape[0]]\n",
    "for j in range(3):\n",
    "    f=[]\n",
    "    for i in range(0,sha[j]):\n",
    "        if j==0: \n",
    "            img = X_train[i].reshape(32,32,3)\n",
    "        else:\n",
    "            if j==1:\n",
    "                img = X_val[i].reshape(32,32,3)\n",
    "            else:\n",
    "                img = X_test[i].reshape(32,32,3)\n",
    "                \n",
    "        img = np.mean(np.double(img),axis=2)\n",
    "        feaArr = extractor.process_image(img)\n",
    "        feaArr = feaArr.flatten()\n",
    "        f.append(feaArr)\n",
    "        if i%1000 == 0 :\n",
    "            print str(i)+\" images extracted\"\n",
    "    fe.append(f)\n",
    "print np.array(f).shape\n",
    "X_train_feats = fe[0]\n",
    "X_val_feats = fe[1]\n",
    "X_test_feats = fe[2]\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat\n",
    "X_val_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_val_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM on features\n",
    "Using the multiclass SVM code developed earlier in the assignment, train SVMs on top of the features extracted above; this should achieve better results than training SVMs directly on top of raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 66.260143\n",
      "iteration 100 / 2000: loss 65.079175\n",
      "iteration 200 / 2000: loss 64.028854\n",
      "iteration 300 / 2000: loss 62.879741\n",
      "iteration 400 / 2000: loss 61.823113\n",
      "iteration 500 / 2000: loss 60.808731\n",
      "iteration 600 / 2000: loss 59.755392\n",
      "iteration 700 / 2000: loss 58.751626\n",
      "iteration 800 / 2000: loss 57.811802\n",
      "iteration 900 / 2000: loss 56.798424\n",
      "iteration 1000 / 2000: loss 55.864402\n",
      "iteration 1100 / 2000: loss 54.928443\n",
      "iteration 1200 / 2000: loss 54.021427\n",
      "iteration 1300 / 2000: loss 53.142665\n",
      "iteration 1400 / 2000: loss 52.243390\n",
      "iteration 1500 / 2000: loss 51.437374\n",
      "iteration 1600 / 2000: loss 50.558399\n",
      "iteration 1700 / 2000: loss 49.767724\n",
      "iteration 1800 / 2000: loss 48.907608\n",
      "iteration 1900 / 2000: loss 48.126269\n",
      "0.107\n",
      "iteration 0 / 2000: loss 585.048566\n",
      "iteration 100 / 2000: loss 480.568386\n",
      "iteration 200 / 2000: loss 395.044529\n",
      "iteration 300 / 2000: loss 325.048251\n",
      "iteration 400 / 2000: loss 267.734465\n",
      "iteration 500 / 2000: loss 220.803794\n",
      "iteration 600 / 2000: loss 182.391606\n",
      "iteration 700 / 2000: loss 150.958910\n",
      "iteration 800 / 2000: loss 125.193829\n",
      "iteration 900 / 2000: loss 104.120566\n",
      "iteration 1000 / 2000: loss 86.891831\n",
      "iteration 1100 / 2000: loss 72.760412\n",
      "iteration 1200 / 2000: loss 61.195075\n",
      "iteration 1300 / 2000: loss 51.713263\n",
      "iteration 1400 / 2000: loss 43.976721\n",
      "iteration 1500 / 2000: loss 37.638906\n",
      "iteration 1600 / 2000: loss 32.445127\n",
      "iteration 1700 / 2000: loss 28.193417\n",
      "iteration 1800 / 2000: loss 24.702494\n",
      "iteration 1900 / 2000: loss 21.858153\n",
      "0.113\n",
      "iteration 0 / 2000: loss 2869.680397\n",
      "iteration 100 / 2000: loss 1058.749932\n",
      "iteration 200 / 2000: loss 394.219278\n",
      "iteration 300 / 2000: loss 150.351300\n",
      "iteration 400 / 2000: loss 60.870746\n",
      "iteration 500 / 2000: loss 28.031884\n",
      "iteration 600 / 2000: loss 15.982435\n",
      "iteration 700 / 2000: loss 11.564107\n",
      "iteration 800 / 2000: loss 9.940160\n",
      "iteration 900 / 2000: loss 9.345026\n",
      "iteration 1000 / 2000: loss 9.126118\n",
      "iteration 1100 / 2000: loss 9.045954\n",
      "iteration 1200 / 2000: loss 9.016530\n",
      "iteration 1300 / 2000: loss 9.005908\n",
      "iteration 1400 / 2000: loss 9.001811\n",
      "iteration 1500 / 2000: loss 9.000496\n",
      "iteration 1600 / 2000: loss 8.999889\n",
      "iteration 1700 / 2000: loss 8.999620\n",
      "iteration 1800 / 2000: loss 8.999653\n",
      "iteration 1900 / 2000: loss 8.999437\n",
      "0.392\n",
      "iteration 0 / 2000: loss 5848.027844\n",
      "iteration 100 / 2000: loss 791.306524\n",
      "iteration 200 / 2000: loss 113.810363\n",
      "iteration 300 / 2000: loss 23.043591\n",
      "iteration 400 / 2000: loss 10.881091\n",
      "iteration 500 / 2000: loss 9.251797\n",
      "iteration 600 / 2000: loss 9.033502\n",
      "iteration 700 / 2000: loss 9.004313\n",
      "iteration 800 / 2000: loss 9.000412\n",
      "iteration 900 / 2000: loss 8.999855\n",
      "iteration 1000 / 2000: loss 8.999761\n",
      "iteration 1100 / 2000: loss 8.999786\n",
      "iteration 1200 / 2000: loss 8.999766\n",
      "iteration 1300 / 2000: loss 8.999779\n",
      "iteration 1400 / 2000: loss 8.999835\n",
      "iteration 1500 / 2000: loss 8.999825\n",
      "iteration 1600 / 2000: loss 8.999848\n",
      "iteration 1700 / 2000: loss 8.999800\n",
      "iteration 1800 / 2000: loss 8.999773\n",
      "iteration 1900 / 2000: loss 8.999804\n",
      "0.396\n",
      "iteration 0 / 2000: loss 67.252318\n",
      "iteration 100 / 2000: loss 56.698279\n",
      "iteration 200 / 2000: loss 48.062867\n",
      "iteration 300 / 2000: loss 40.937975\n",
      "iteration 400 / 2000: loss 35.151870\n",
      "iteration 500 / 2000: loss 30.420899\n",
      "iteration 600 / 2000: loss 26.509928\n",
      "iteration 700 / 2000: loss 23.328793\n",
      "iteration 800 / 2000: loss 20.729963\n",
      "iteration 900 / 2000: loss 18.603393\n",
      "iteration 1000 / 2000: loss 16.851390\n",
      "iteration 1100 / 2000: loss 15.439973\n",
      "iteration 1200 / 2000: loss 14.260205\n",
      "iteration 1300 / 2000: loss 13.311745\n",
      "iteration 1400 / 2000: loss 12.515798\n",
      "iteration 1500 / 2000: loss 11.886706\n",
      "iteration 1600 / 2000: loss 11.349867\n",
      "iteration 1700 / 2000: loss 10.912414\n",
      "iteration 1800 / 2000: loss 10.558346\n",
      "iteration 1900 / 2000: loss 10.280427\n",
      "0.243\n",
      "iteration 0 / 2000: loss 589.625160\n",
      "iteration 100 / 2000: loss 86.799423\n",
      "iteration 200 / 2000: loss 19.416823\n",
      "iteration 300 / 2000: loss 10.392179\n",
      "iteration 400 / 2000: loss 9.185719\n",
      "iteration 500 / 2000: loss 9.022849\n",
      "iteration 600 / 2000: loss 9.000779\n",
      "iteration 700 / 2000: loss 8.998063\n",
      "iteration 800 / 2000: loss 8.997950\n",
      "iteration 900 / 2000: loss 8.997718\n",
      "iteration 1000 / 2000: loss 8.998247\n",
      "iteration 1100 / 2000: loss 8.997920\n",
      "iteration 1200 / 2000: loss 8.998068\n",
      "iteration 1300 / 2000: loss 8.998196\n",
      "iteration 1400 / 2000: loss 8.997731\n",
      "iteration 1500 / 2000: loss 8.997566\n",
      "iteration 1600 / 2000: loss 8.997863\n",
      "iteration 1700 / 2000: loss 8.997713\n",
      "iteration 1800 / 2000: loss 8.998029\n",
      "iteration 1900 / 2000: loss 8.997697\n",
      "0.392\n",
      "iteration 0 / 2000: loss 2876.613637\n",
      "iteration 100 / 2000: loss 9.100305\n",
      "iteration 200 / 2000: loss 8.999643\n",
      "iteration 300 / 2000: loss 8.999592\n",
      "iteration 400 / 2000: loss 8.999582\n",
      "iteration 500 / 2000: loss 8.999663\n",
      "iteration 600 / 2000: loss 8.999572\n",
      "iteration 700 / 2000: loss 8.999587\n",
      "iteration 800 / 2000: loss 8.999612\n",
      "iteration 900 / 2000: loss 8.999700\n",
      "iteration 1000 / 2000: loss 8.999573\n",
      "iteration 1100 / 2000: loss 8.999630\n",
      "iteration 1200 / 2000: loss 8.999690\n",
      "iteration 1300 / 2000: loss 8.999607\n",
      "iteration 1400 / 2000: loss 8.999633\n",
      "iteration 1500 / 2000: loss 8.999633\n",
      "iteration 1600 / 2000: loss 8.999603\n",
      "iteration 1700 / 2000: loss 8.999548\n",
      "iteration 1800 / 2000: loss 8.999471\n",
      "iteration 1900 / 2000: loss 8.999615\n",
      "0.396\n",
      "iteration 0 / 2000: loss 5859.687460\n",
      "iteration 100 / 2000: loss 8.999786\n",
      "iteration 200 / 2000: loss 8.999750\n",
      "iteration 300 / 2000: loss 8.999872\n",
      "iteration 400 / 2000: loss 8.999789\n",
      "iteration 500 / 2000: loss 8.999737\n",
      "iteration 600 / 2000: loss 8.999788\n",
      "iteration 700 / 2000: loss 8.999769\n",
      "iteration 800 / 2000: loss 8.999854\n",
      "iteration 900 / 2000: loss 8.999799\n",
      "iteration 1000 / 2000: loss 8.999803\n",
      "iteration 1100 / 2000: loss 8.999833\n",
      "iteration 1200 / 2000: loss 8.999817\n",
      "iteration 1300 / 2000: loss 8.999785\n",
      "iteration 1400 / 2000: loss 8.999812\n",
      "iteration 1500 / 2000: loss 8.999831\n",
      "iteration 1600 / 2000: loss 8.999824\n",
      "iteration 1700 / 2000: loss 8.999783\n",
      "iteration 1800 / 2000: loss 8.999786\n",
      "iteration 1900 / 2000: loss 8.999822\n",
      "0.394\n",
      "iteration 0 / 2000: loss 67.995805\n",
      "iteration 100 / 2000: loss 16.879255\n",
      "iteration 200 / 2000: loss 10.041469\n",
      "iteration 300 / 2000: loss 9.118181\n",
      "iteration 400 / 2000: loss 8.994276\n",
      "iteration 500 / 2000: loss 8.982357\n",
      "iteration 600 / 2000: loss 8.978032\n",
      "iteration 700 / 2000: loss 8.979058\n",
      "iteration 800 / 2000: loss 8.975684\n",
      "iteration 900 / 2000: loss 8.974710\n",
      "iteration 1000 / 2000: loss 8.975599\n",
      "iteration 1100 / 2000: loss 8.976281\n",
      "iteration 1200 / 2000: loss 8.975786\n",
      "iteration 1300 / 2000: loss 8.983509\n",
      "iteration 1400 / 2000: loss 8.979762\n",
      "iteration 1500 / 2000: loss 8.975894\n",
      "iteration 1600 / 2000: loss 8.979052\n",
      "iteration 1700 / 2000: loss 8.983408\n",
      "iteration 1800 / 2000: loss 8.981218\n",
      "iteration 1900 / 2000: loss 8.976524\n",
      "0.395\n",
      "iteration 0 / 2000: loss 591.191063\n",
      "iteration 100 / 2000: loss 8.998336\n",
      "iteration 200 / 2000: loss 8.997961\n",
      "iteration 300 / 2000: loss 8.998807\n",
      "iteration 400 / 2000: loss 8.998134\n",
      "iteration 500 / 2000: loss 8.998327\n",
      "iteration 600 / 2000: loss 8.998268\n",
      "iteration 700 / 2000: loss 8.998481\n",
      "iteration 800 / 2000: loss 8.997939\n",
      "iteration 900 / 2000: loss 8.998490\n",
      "iteration 1000 / 2000: loss 8.998411\n",
      "iteration 1100 / 2000: loss 8.998362\n",
      "iteration 1200 / 2000: loss 8.998207\n",
      "iteration 1300 / 2000: loss 8.997903\n",
      "iteration 1400 / 2000: loss 8.998031\n",
      "iteration 1500 / 2000: loss 8.998253\n",
      "iteration 1600 / 2000: loss 8.998114\n",
      "iteration 1700 / 2000: loss 8.998253\n",
      "iteration 1800 / 2000: loss 8.998577\n",
      "iteration 1900 / 2000: loss 8.998255\n",
      "0.392\n",
      "iteration 0 / 2000: loss 2875.231550\n",
      "iteration 100 / 2000: loss 8.999784\n",
      "iteration 200 / 2000: loss 8.999840\n",
      "iteration 300 / 2000: loss 8.999793\n",
      "iteration 400 / 2000: loss 8.999584\n",
      "iteration 500 / 2000: loss 8.999766\n",
      "iteration 600 / 2000: loss 8.999757\n",
      "iteration 700 / 2000: loss 8.999831\n",
      "iteration 800 / 2000: loss 8.999695\n",
      "iteration 900 / 2000: loss 8.999740\n",
      "iteration 1000 / 2000: loss 8.999801\n",
      "iteration 1100 / 2000: loss 8.999807\n",
      "iteration 1200 / 2000: loss 8.999746\n",
      "iteration 1300 / 2000: loss 8.999647\n",
      "iteration 1400 / 2000: loss 8.999762\n",
      "iteration 1500 / 2000: loss 8.999857\n",
      "iteration 1600 / 2000: loss 8.999768\n",
      "iteration 1700 / 2000: loss 8.999772\n",
      "iteration 1800 / 2000: loss 8.999756\n",
      "iteration 1900 / 2000: loss 8.999804\n",
      "0.359\n",
      "iteration 0 / 2000: loss 5816.743306\n",
      "iteration 100 / 2000: loss 9.000090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 2000: loss 9.000056\n",
      "iteration 300 / 2000: loss 9.000079\n",
      "iteration 400 / 2000: loss 9.000082\n",
      "iteration 500 / 2000: loss 9.000011\n",
      "iteration 600 / 2000: loss 9.000044\n",
      "iteration 700 / 2000: loss 9.000022\n",
      "iteration 800 / 2000: loss 9.000070\n",
      "iteration 900 / 2000: loss 9.000059\n",
      "iteration 1000 / 2000: loss 9.000050\n",
      "iteration 1100 / 2000: loss 8.999988\n",
      "iteration 1200 / 2000: loss 9.000053\n",
      "iteration 1300 / 2000: loss 9.000043\n",
      "iteration 1400 / 2000: loss 9.000033\n",
      "iteration 1500 / 2000: loss 9.000110\n",
      "iteration 1600 / 2000: loss 9.000032\n",
      "iteration 1700 / 2000: loss 9.000022\n",
      "iteration 1800 / 2000: loss 9.000003\n",
      "iteration 1900 / 2000: loss 9.000074\n",
      "0.328\n",
      "lr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.094531 val accuracy: 0.107000\n",
      "lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.130041 val accuracy: 0.113000\n",
      "lr 1.000000e-08 reg 5.000000e+05 train accuracy: 0.387918 val accuracy: 0.392000\n",
      "lr 1.000000e-08 reg 1.000000e+06 train accuracy: 0.387592 val accuracy: 0.396000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.238857 val accuracy: 0.243000\n",
      "lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.388286 val accuracy: 0.392000\n",
      "lr 1.000000e-07 reg 5.000000e+05 train accuracy: 0.390224 val accuracy: 0.396000\n",
      "lr 1.000000e-07 reg 1.000000e+06 train accuracy: 0.388204 val accuracy: 0.394000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.389735 val accuracy: 0.395000\n",
      "lr 1.000000e-06 reg 1.000000e+05 train accuracy: 0.389898 val accuracy: 0.392000\n",
      "lr 1.000000e-06 reg 5.000000e+05 train accuracy: 0.351041 val accuracy: 0.359000\n",
      "lr 1.000000e-06 reg 1.000000e+06 train accuracy: 0.317204 val accuracy: 0.328000\n",
      "best validation accuracy achieved during cross-validation: 0.396000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune the learning rate and regularization strength\n",
    "\n",
    "from asgn1.classifiers.linear_classifier import LinearSVM\n",
    "\n",
    "X_train_feats=np.array(X_train_feats)\n",
    "X_val_feats=np.array(X_val_feats)\n",
    "\n",
    "learning_rates = [1e-8, 1e-7,1e-6]\n",
    "regularization_strengths = [1e4,1e5,5e5,1e6]\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_svm = None\n",
    "for i in learning_rates:\n",
    "    for j in regularization_strengths:\n",
    "        svm=LinearSVM()\n",
    "        loss_hist = svm.train(X_train_feats, y_train, learning_rate=i, reg=j,\n",
    "                      num_iters=2000, verbose=True)\n",
    "        y_train_pred = svm.predict(X_train_feats)\n",
    "        y_val_pred = svm.predict(X_val_feats)\n",
    "        results[i,j]=[np.mean(y_train == y_train_pred),np.mean(y_val == y_val_pred)]\n",
    "        print np.mean(y_val == y_val_pred)\n",
    "        if (np.mean(y_val == y_val_pred))>best_val:\n",
    "            best_val=np.mean(y_val == y_val_pred)\n",
    "            best_svm=svm\n",
    "\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.417\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your trained SVM on the test set\n",
    "y_test_pred = best_svm.predict(np.array(X_test_feats))\n",
    "confmatrix = np.zeros((num_classes,num_classes))\n",
    "for i in range(0,y_test_pred.shape[0]):\n",
    "    confmatrix[y_test[i],y_test_pred[i]]+=1\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({\"best_svm\":best_svm,\"cmatrix\":confmatrix}, open( \"best_svm_SIFT.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network on dSIFT image features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 1153)\n"
     ]
    }
   ],
   "source": [
    "print X_train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1600: loss 2.302585\n",
      "iteration 100 / 1600: loss 1.544467\n",
      "iteration 200 / 1600: loss 1.318993\n",
      "iteration 300 / 1600: loss 1.216357\n",
      "iteration 400 / 1600: loss 1.226037\n",
      "iteration 500 / 1600: loss 1.076611\n",
      "iteration 600 / 1600: loss 1.040595\n",
      "iteration 700 / 1600: loss 1.050253\n",
      "iteration 800 / 1600: loss 0.965504\n",
      "iteration 900 / 1600: loss 1.161479\n",
      "iteration 1000 / 1600: loss 1.101231\n",
      "iteration 1100 / 1600: loss 1.005848\n",
      "iteration 1200 / 1600: loss 0.863756\n",
      "iteration 1300 / 1600: loss 0.856009\n",
      "iteration 1400 / 1600: loss 0.907827\n",
      "iteration 1500 / 1600: loss 0.906499\n",
      "0.609\n",
      "iteration 0 / 1600: loss 2.302586\n",
      "iteration 100 / 1600: loss 1.688927\n",
      "iteration 200 / 1600: loss 1.456226\n",
      "iteration 300 / 1600: loss 1.299109\n",
      "iteration 400 / 1600: loss 1.130677\n",
      "iteration 500 / 1600: loss 1.186749\n",
      "iteration 600 / 1600: loss 1.124501\n",
      "iteration 700 / 1600: loss 1.181849\n",
      "iteration 800 / 1600: loss 1.136514\n",
      "iteration 900 / 1600: loss 1.108008\n",
      "iteration 1000 / 1600: loss 1.061629\n",
      "iteration 1100 / 1600: loss 1.128128\n",
      "iteration 1200 / 1600: loss 1.033161\n",
      "iteration 1300 / 1600: loss 1.122909\n",
      "iteration 1400 / 1600: loss 0.936440\n",
      "iteration 1500 / 1600: loss 1.126174\n",
      "0.593\n",
      "iteration 0 / 1600: loss 2.302586\n",
      "iteration 100 / 1600: loss 1.578235\n",
      "iteration 200 / 1600: loss 1.371566\n",
      "iteration 300 / 1600: loss 1.295205\n",
      "iteration 400 / 1600: loss 1.483412\n",
      "iteration 500 / 1600: loss 1.171200\n",
      "iteration 600 / 1600: loss 1.106037\n",
      "iteration 700 / 1600: loss 1.140160\n",
      "iteration 800 / 1600: loss 1.156868\n",
      "iteration 900 / 1600: loss 1.161186\n",
      "iteration 1000 / 1600: loss 1.083050\n",
      "iteration 1100 / 1600: loss 1.068730\n",
      "iteration 1200 / 1600: loss 0.989825\n",
      "iteration 1300 / 1600: loss 0.967921\n",
      "iteration 1400 / 1600: loss 1.094253\n",
      "iteration 1500 / 1600: loss 1.056224\n",
      "0.594\n",
      "iteration 0 / 1600: loss 2.302587\n",
      "iteration 100 / 1600: loss 1.625229\n",
      "iteration 200 / 1600: loss 1.397274\n",
      "iteration 300 / 1600: loss 1.341805\n",
      "iteration 400 / 1600: loss 1.163100\n",
      "iteration 500 / 1600: loss 1.343639\n",
      "iteration 600 / 1600: loss 1.243041\n",
      "iteration 700 / 1600: loss 1.067656\n",
      "iteration 800 / 1600: loss 1.132308\n",
      "iteration 900 / 1600: loss 1.182554\n",
      "iteration 1000 / 1600: loss 1.119410\n",
      "iteration 1100 / 1600: loss 1.144736\n",
      "iteration 1200 / 1600: loss 1.104969\n",
      "iteration 1300 / 1600: loss 1.218751\n",
      "iteration 1400 / 1600: loss 1.141670\n",
      "iteration 1500 / 1600: loss 1.174572\n",
      "0.608\n",
      "iteration 0 / 1600: loss 2.302589\n",
      "iteration 100 / 1600: loss 1.674493\n",
      "iteration 200 / 1600: loss 1.451410\n",
      "iteration 300 / 1600: loss 1.512693\n",
      "iteration 400 / 1600: loss 1.423981\n",
      "iteration 500 / 1600: loss 1.257457\n",
      "iteration 600 / 1600: loss 1.316734\n",
      "iteration 700 / 1600: loss 1.219030\n",
      "iteration 800 / 1600: loss 1.331601\n",
      "iteration 900 / 1600: loss 1.231620\n",
      "iteration 1000 / 1600: loss 1.281611\n",
      "iteration 1100 / 1600: loss 1.176474\n",
      "iteration 1200 / 1600: loss 1.102591\n",
      "iteration 1300 / 1600: loss 1.269775\n",
      "iteration 1400 / 1600: loss 1.332541\n",
      "iteration 1500 / 1600: loss 1.439002\n",
      "0.588\n",
      "iteration 0 / 1600: loss 2.302585\n",
      "iteration 100 / 1600: loss 1.421468\n",
      "iteration 200 / 1600: loss 1.214362\n",
      "iteration 300 / 1600: loss 1.097649\n",
      "iteration 400 / 1600: loss 1.197446\n",
      "iteration 500 / 1600: loss 1.168796\n",
      "iteration 600 / 1600: loss 1.090450\n",
      "iteration 700 / 1600: loss 1.101127\n",
      "iteration 800 / 1600: loss 1.019986\n",
      "iteration 900 / 1600: loss 0.979969\n",
      "iteration 1000 / 1600: loss 1.032960\n",
      "iteration 1100 / 1600: loss 0.900392\n",
      "iteration 1200 / 1600: loss 0.987229\n",
      "iteration 1300 / 1600: loss 0.959874\n",
      "iteration 1400 / 1600: loss 0.914223\n",
      "iteration 1500 / 1600: loss 1.008033\n",
      "0.6\n",
      "iteration 0 / 1600: loss 2.302586\n",
      "iteration 100 / 1600: loss 1.454124\n",
      "iteration 200 / 1600: loss 1.353588\n",
      "iteration 300 / 1600: loss 1.320782\n",
      "iteration 400 / 1600: loss 1.140724\n",
      "iteration 500 / 1600: loss 1.252819\n",
      "iteration 600 / 1600: loss 1.056634\n",
      "iteration 700 / 1600: loss 1.186511\n",
      "iteration 800 / 1600: loss 1.087000\n",
      "iteration 900 / 1600: loss 1.088061\n",
      "iteration 1000 / 1600: loss 1.066196\n",
      "iteration 1100 / 1600: loss 1.045795\n",
      "iteration 1200 / 1600: loss 0.986653\n",
      "iteration 1300 / 1600: loss 0.982197\n",
      "iteration 1400 / 1600: loss 1.149348\n",
      "iteration 1500 / 1600: loss 1.075890\n",
      "0.599\n",
      "iteration 0 / 1600: loss 2.302586\n",
      "iteration 100 / 1600: loss 1.460065\n",
      "iteration 200 / 1600: loss 1.217638\n",
      "iteration 300 / 1600: loss 1.288947\n",
      "iteration 400 / 1600: loss 1.116310\n",
      "iteration 500 / 1600: loss 1.179383\n",
      "iteration 600 / 1600: loss 1.228498\n",
      "iteration 700 / 1600: loss 1.304131\n",
      "iteration 800 / 1600: loss 1.167972\n",
      "iteration 900 / 1600: loss 1.213965\n",
      "iteration 1000 / 1600: loss 1.151424\n",
      "iteration 1100 / 1600: loss 1.021241\n",
      "iteration 1200 / 1600: loss 1.174091\n",
      "iteration 1300 / 1600: loss 1.058807\n",
      "iteration 1400 / 1600: loss 1.182254\n",
      "iteration 1500 / 1600: loss 1.054181\n",
      "0.601\n",
      "iteration 0 / 1600: loss 2.302587\n",
      "iteration 100 / 1600: loss 1.445605\n",
      "iteration 200 / 1600: loss 1.434407\n",
      "iteration 300 / 1600: loss 1.528875\n",
      "iteration 400 / 1600: loss 1.276891\n",
      "iteration 500 / 1600: loss 1.323031\n",
      "iteration 600 / 1600: loss 1.195338\n",
      "iteration 700 / 1600: loss 1.145041\n",
      "iteration 800 / 1600: loss 1.276744\n",
      "iteration 900 / 1600: loss 1.066434\n",
      "iteration 1000 / 1600: loss 1.109340\n",
      "iteration 1100 / 1600: loss 1.121323\n",
      "iteration 1200 / 1600: loss 1.256035\n",
      "iteration 1300 / 1600: loss 1.124413\n",
      "iteration 1400 / 1600: loss 1.184477\n",
      "iteration 1500 / 1600: loss 1.146032\n",
      "0.597\n",
      "iteration 0 / 1600: loss 2.302589\n",
      "iteration 100 / 1600: loss 1.491854\n",
      "iteration 200 / 1600: loss 1.439815\n",
      "iteration 300 / 1600: loss 1.389324\n",
      "iteration 400 / 1600: loss 1.238489\n",
      "iteration 500 / 1600: loss 1.279129\n",
      "iteration 600 / 1600: loss 1.302218\n",
      "iteration 700 / 1600: loss 1.357703\n",
      "iteration 800 / 1600: loss 1.420489\n",
      "iteration 900 / 1600: loss 1.275781\n",
      "iteration 1000 / 1600: loss 1.309946\n",
      "iteration 1100 / 1600: loss 1.231583\n",
      "iteration 1200 / 1600: loss 1.332781\n",
      "iteration 1300 / 1600: loss 1.353984\n",
      "iteration 1400 / 1600: loss 1.249848\n",
      "iteration 1500 / 1600: loss 1.293598\n",
      "0.588\n",
      "iteration 0 / 1600: loss 2.302585\n",
      "iteration 100 / 1600: loss 1.322960\n",
      "iteration 200 / 1600: loss 1.384708\n",
      "iteration 300 / 1600: loss 1.546000\n",
      "iteration 400 / 1600: loss 1.063676\n",
      "iteration 500 / 1600: loss 1.341313\n",
      "iteration 600 / 1600: loss 1.091442\n",
      "iteration 700 / 1600: loss 1.076446\n",
      "iteration 800 / 1600: loss 1.122037\n",
      "iteration 900 / 1600: loss 1.035637\n",
      "iteration 1000 / 1600: loss 0.932285\n",
      "iteration 1100 / 1600: loss 1.230215\n",
      "iteration 1200 / 1600: loss 1.200951\n",
      "iteration 1300 / 1600: loss 0.991326\n",
      "iteration 1400 / 1600: loss 0.949689\n",
      "iteration 1500 / 1600: loss 0.923563\n",
      "0.604\n",
      "iteration 0 / 1600: loss 2.302586\n",
      "iteration 100 / 1600: loss 1.592241\n",
      "iteration 200 / 1600: loss 1.449302\n",
      "iteration 300 / 1600: loss 1.353580\n",
      "iteration 400 / 1600: loss 1.269780\n",
      "iteration 500 / 1600: loss 1.235001\n",
      "iteration 600 / 1600: loss 1.218877\n",
      "iteration 700 / 1600: loss 1.320393\n",
      "iteration 800 / 1600: loss 0.991970\n",
      "iteration 900 / 1600: loss 1.024486\n",
      "iteration 1000 / 1600: loss 1.115593\n",
      "iteration 1100 / 1600: loss 1.130112\n",
      "iteration 1200 / 1600: loss 1.393333\n",
      "iteration 1300 / 1600: loss 1.175247\n",
      "iteration 1400 / 1600: loss 1.015562\n",
      "iteration 1500 / 1600: loss 0.917492\n",
      "0.58\n",
      "iteration 0 / 1600: loss 2.302586\n",
      "iteration 100 / 1600: loss 1.575920\n",
      "iteration 200 / 1600: loss 1.448360\n",
      "iteration 300 / 1600: loss 1.371973\n",
      "iteration 400 / 1600: loss 1.326289\n",
      "iteration 500 / 1600: loss 1.215304\n",
      "iteration 600 / 1600: loss 1.263154\n",
      "iteration 700 / 1600: loss 1.248485\n",
      "iteration 800 / 1600: loss 1.291457\n",
      "iteration 900 / 1600: loss 1.321722\n",
      "iteration 1000 / 1600: loss 1.181831\n",
      "iteration 1100 / 1600: loss 1.232942\n",
      "iteration 1200 / 1600: loss 1.222472\n",
      "iteration 1300 / 1600: loss 1.308973\n",
      "iteration 1400 / 1600: loss 1.104347\n",
      "iteration 1500 / 1600: loss 1.069149\n",
      "0.581\n",
      "iteration 0 / 1600: loss 2.302587\n",
      "iteration 100 / 1600: loss 1.706600\n",
      "iteration 200 / 1600: loss 1.394163\n",
      "iteration 300 / 1600: loss 1.421458\n",
      "iteration 400 / 1600: loss 1.406547\n",
      "iteration 500 / 1600: loss 1.219260\n",
      "iteration 600 / 1600: loss 1.281779\n",
      "iteration 700 / 1600: loss 1.277649\n",
      "iteration 800 / 1600: loss 1.404785\n",
      "iteration 900 / 1600: loss 1.276668\n",
      "iteration 1000 / 1600: loss 1.189982\n",
      "iteration 1100 / 1600: loss 1.296930\n",
      "iteration 1200 / 1600: loss 1.154120\n",
      "iteration 1300 / 1600: loss 1.055319\n",
      "iteration 1400 / 1600: loss 1.250271\n",
      "iteration 1500 / 1600: loss 1.278105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.596\n",
      "iteration 0 / 1600: loss 2.302589\n",
      "iteration 100 / 1600: loss 1.448800\n",
      "iteration 200 / 1600: loss 1.522179\n",
      "iteration 300 / 1600: loss 1.422487\n",
      "iteration 400 / 1600: loss 1.471589\n",
      "iteration 500 / 1600: loss 1.350842\n",
      "iteration 600 / 1600: loss 1.448632\n",
      "iteration 700 / 1600: loss 1.379315\n",
      "iteration 800 / 1600: loss 1.302109\n",
      "iteration 900 / 1600: loss 1.437690\n",
      "iteration 1000 / 1600: loss 1.432704\n",
      "iteration 1100 / 1600: loss 1.536419\n",
      "iteration 1200 / 1600: loss 1.462488\n",
      "iteration 1300 / 1600: loss 1.322823\n",
      "iteration 1400 / 1600: loss 1.434752\n",
      "iteration 1500 / 1600: loss 1.343766\n",
      "0.581\n",
      "0.609\n",
      "lr 2.000000e-01 reg 5.000000e-04 train accuracy: 0.692020 val accuracy: 0.609000\n",
      "lr 2.000000e-01 reg 2.000000e-03 train accuracy: 0.684000 val accuracy: 0.593000\n",
      "lr 2.000000e-01 reg 3.000000e-03 train accuracy: 0.678531 val accuracy: 0.594000\n",
      "lr 2.000000e-01 reg 5.000000e-03 train accuracy: 0.672102 val accuracy: 0.608000\n",
      "lr 2.000000e-01 reg 1.000000e-02 train accuracy: 0.645490 val accuracy: 0.588000\n",
      "lr 3.000000e-01 reg 5.000000e-04 train accuracy: 0.691347 val accuracy: 0.600000\n",
      "lr 3.000000e-01 reg 2.000000e-03 train accuracy: 0.688510 val accuracy: 0.599000\n",
      "lr 3.000000e-01 reg 3.000000e-03 train accuracy: 0.674490 val accuracy: 0.601000\n",
      "lr 3.000000e-01 reg 5.000000e-03 train accuracy: 0.668469 val accuracy: 0.597000\n",
      "lr 3.000000e-01 reg 1.000000e-02 train accuracy: 0.636939 val accuracy: 0.588000\n",
      "lr 5.000000e-01 reg 5.000000e-04 train accuracy: 0.696286 val accuracy: 0.604000\n",
      "lr 5.000000e-01 reg 2.000000e-03 train accuracy: 0.678735 val accuracy: 0.580000\n",
      "lr 5.000000e-01 reg 3.000000e-03 train accuracy: 0.661633 val accuracy: 0.581000\n",
      "lr 5.000000e-01 reg 5.000000e-03 train accuracy: 0.656714 val accuracy: 0.596000\n",
      "lr 5.000000e-01 reg 1.000000e-02 train accuracy: 0.620143 val accuracy: 0.581000\n",
      "best validation accuracy achieved during cross-validation: 0.609000\n"
     ]
    }
   ],
   "source": [
    "from asgn1.classifiers.neural_net import TwoLayerNet\n",
    "\n",
    "input_dim = X_train_feats.shape[1]\n",
    "hidden_dim = 70\n",
    "num_classes = 10\n",
    "learning_rates=[2e-1,3e-1,5e-1]\n",
    "regularization_strengths = [0.0005,0.002,0.003,0.005,0.01]\n",
    "best_val=-1\n",
    "results={}\n",
    "best_net = None\n",
    "input_size = 32 * 32 * 3\n",
    "for i in learning_rates:\n",
    "    for j in regularization_strengths:\n",
    "        net = TwoLayerNet(input_dim, hidden_dim, num_classes)\n",
    "        stats = net.train(X_train_feats, y_train, X_val_feats, y_val,\n",
    "                                num_iters=1600,batch_size=200,\n",
    "                                learning_rate=i, learning_rate_decay=0.95,\n",
    "                                    reg=j, verbose=True)\n",
    "        y_train_pred = net.predict(X_train_feats)\n",
    "        y_val_pred = net.predict(X_val_feats)\n",
    "        results[i,j]=[np.mean(y_train == y_train_pred),np.mean(y_val == y_val_pred)]\n",
    "        print np.mean(y_val == y_val_pred)\n",
    "        if (np.mean(y_val == y_val_pred))>best_val:\n",
    "            best_val=np.mean(y_val == y_val_pred)\n",
    "            best_net=net\n",
    "print best_val\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.602\n"
     ]
    }
   ],
   "source": [
    "# Run your neural net classifier on the test set. You should be able to\n",
    "# get more than 55% accuracy.\n",
    "confmatrix = np.zeros((num_classes,num_classes))\n",
    "for i in range(0,y_test_pred.shape[0]):\n",
    "    confmatrix[y_test[i],y_test_pred[i]]+=1\n",
    "test_acc = (best_net.predict(np.array(X_test_feats)) == y_test).mean()\n",
    "print test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Acheived 60.2% accuracy when using dSift features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the best svm model to pickel file\n",
    "import pickle\n",
    "pickle.dump({\"best_net\":best_net,\"cmatrix\":confmatrix}, open( \"best_net_SIFT.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
